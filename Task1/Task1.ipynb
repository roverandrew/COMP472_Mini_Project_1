{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "25601a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up imports and load the data\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb51d8c",
   "metadata": {},
   "source": [
    "2. Plot the distribution of the instances in each class and save the graphic in a file called BBC-distribution.pdf. You may want to use matplotlib.pyplot and savefig to do this. This pre-analysis of the data set will allow you to determine if the classes are balanced, and which metric is more appropriate to use to evaluate the performance of your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f335cd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "category_distribution = {'business':0, 'entertainment':0, 'politics':0, 'sport':0, 'tech':0};\n",
    "BBC_data_dir='../data/BBC';\n",
    "\n",
    "for name in os.listdir(BBC_data_dir):\n",
    "    if name in category_distribution.keys():\n",
    "        category_dir = os.path.join(BBC_data_dir, name)\n",
    "        files_in_directory = next(os.walk(category_dir))[2]\n",
    "        category_distribution[name] = len(files_in_directory)\n",
    "        \n",
    "fig = plt.figure()\n",
    "plt.bar(\n",
    "    category_distribution.keys(),\n",
    "    category_distribution.values(), \n",
    ");\n",
    "\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Number of Files')\n",
    "plt.title('BBC Data Distribution')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"BBC-distribution.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fee5be",
   "metadata": {},
   "source": [
    "3. Load the corpus using load files and make sure you set the encoding to latin1. This will read the file structure and assign the category name to each file from their parent directory name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776a8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load all the data from the files.\n",
    "corpus = load_files(\n",
    "    container_path='../data/BBC',\n",
    "    description='Reference: D. Greene and P. Cunningham.\"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006.',\n",
    "    encoding='latin1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f86a14",
   "metadata": {},
   "source": [
    "4. Pre-process the dataset to have the features ready to be used by a multinomial Naive Bayes classifier. This means  that the frequency of each word in each class must be computed and stored in a term-document matrix. For this, you can use feature extraction.text.CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc271d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Pre-process the data. Returns a dictionary, mapping terms to feature indices\n",
    "vectorizer = CountVectorizer(input='content', encoding='latin1') # Not sure if encoding needs to be latin1 here as well.\n",
    "term_document_dictionary = vectorizer.fit_transform(corpus.data)\n",
    "term_document_matrix = term_document_dictionary.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ae2391",
   "metadata": {},
   "source": [
    "5. Split the dataset into 80% for training and 20% for testing. For this, you must use train test split with\n",
    "   the parameter random state set to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bdeda78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Split the data\n",
    "target_labels = corpus.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    term_document_matrix, # Not sure if this should be term_document_dictionary\n",
    "    target_labels,\n",
    "    train_size=0.8, \n",
    "    test_size=0.2,\n",
    "    random_state=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644028b",
   "metadata": {},
   "source": [
    "6. Train a multinomial Naive Bayes Classifier (naive bayes.MultinomialNB) on the training set using the\n",
    "   default parameters and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53a9076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train (Calculate the probabilties)\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Use above calculated probabilities on the test set.\n",
    "y_naive_bayes_predict = naive_bayes_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c866e409",
   "metadata": {},
   "source": [
    "7. b) the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8bcd82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[99  0  3  0  2]\n",
      " [ 0 70  1  0  2]\n",
      " [ 1  0 95  0  0]\n",
      " [ 0  0  1 87  0]\n",
      " [ 0  0  0  0 84]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_naive_bayes_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c6f19",
   "metadata": {},
   "source": [
    "7. c) the precision, recall, and F1-measure for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f290713e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score:\n",
      "[0.99       1.         0.95       1.         0.95454545]\n",
      "\n",
      "recall score:\n",
      "[0.95192308 0.95890411 0.98958333 0.98863636 1.        ]\n",
      "\n",
      "f1-measure:\n",
      "[0.97058824 0.97902098 0.96938776 0.99428571 0.97674419]\n"
     ]
    }
   ],
   "source": [
    "# precision score\n",
    "print(\"precision score:\")\n",
    "print(precision_score(y_test, y_naive_bayes_predict, average=None))\n",
    "\n",
    "# recall score\n",
    "print(\"\\nrecall score:\")\n",
    "print(recall_score(y_test, y_naive_bayes_predict, average=None))\n",
    "\n",
    "# f1-measure\n",
    "print(\"\\nf1-measure:\")\n",
    "print(f1_score(y_test, y_naive_bayes_predict, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c8387",
   "metadata": {},
   "source": [
    "7. d) the accuracy, macro-average F1 and weighted-average F1 of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e5d514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:\n",
      "0.9775280898876404\n",
      "\n",
      "macro-average f1 score:\n",
      "0.9780053739498727\n",
      "\n",
      "weighted-average f1 score:\n",
      "0.9775608694695853\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "print(\"accuracy score:\")\n",
    "print(accuracy_score(y_test, y_naive_bayes_predict))\n",
    "\n",
    "# macro-average f1\n",
    "print(\"\\nmacro-average f1 score:\")\n",
    "print(f1_score(y_test, y_naive_bayes_predict, average='macro'))\n",
    "\n",
    "# weighted-average f1\n",
    "print(\"\\nweighted-average f1 score:\")\n",
    "print(f1_score(y_test, y_naive_bayes_predict, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ea389",
   "metadata": {},
   "source": [
    "7. e) the prior probability of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fef4ba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['business', 'entertainment', 'politics', 'sport', 'tech'])\n",
      "dict_values([510, 386, 417, 511, 401])\n",
      "['business', 'entertainment', 'politics', 'sport', 'tech']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "P(category) = # of documents in category / total # of documents\n",
      "\n",
      "P(business) = 0.22808988764044943\n",
      "P(entertainment) = 0.17584269662921348\n",
      "P(politics) = 0.18033707865168538\n",
      "P(sport) = 0.23764044943820226\n",
      "P(tech) = 0.17808988764044945\n"
     ]
    }
   ],
   "source": [
    "# Not sure about here, there are less documents compared to what is displayed in the generated PDF, however I believe\n",
    "# that makes sense as we use an 80% training split. However the ordering of largest to smallest # of documents/category has changed\n",
    "# for example, sports previously had the most amount of documents, now it has the second most.\n",
    "\n",
    "# Not sure if perhaps the mapping of category_index to target_names is incorrect.\n",
    "\n",
    "print(category_distribution.keys())\n",
    "print(category_distribution.values())\n",
    "print(corpus.target_names)\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "# OR: use class_log_prior_ attribute. Maybe just show both?\n",
    "\n",
    "#### Answer, for now:\n",
    "# prior probability, probability of a category before any evidence is obtained \n",
    "# (i.e.) Probability of a category without looking at it's contents\n",
    "\n",
    "# Calculate total # of documents\n",
    "total_num_of_documents = sum(naive_bayes_classifier.class_count_);\n",
    "print(\"P(category) = # of documents in category / total # of documents\\n\")\n",
    "    \n",
    "for category_index in naive_bayes_classifier.classes_:\n",
    "    print(f\"P({corpus.target_names[category_index]}) = {naive_bayes_classifier.class_count_[category_index]/total_num_of_documents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e3e04c",
   "metadata": {},
   "source": [
    "7. f) the size of the vocabulary (i.e. the number of different words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18e18173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 29421\n"
     ]
    }
   ],
   "source": [
    "#Vocabulary\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2309a06",
   "metadata": {},
   "source": [
    "7. g) the number of word-tokens in each class (i.e. the number of words in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf5e7de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of word-tokens in class business: 131035\n",
      "# of word-tokens in class entertainment: 102443\n",
      "# of word-tokens in class politics: 146012\n",
      "# of word-tokens in class sport: 133730\n",
      "# of word-tokens in class tech: 159565\n"
     ]
    }
   ],
   "source": [
    "total_words_in_corpus=0\n",
    "for category_index in naive_bayes_classifier.classes_:\n",
    "    total_words_in_class=int(sum(naive_bayes_classifier.feature_count_[category_index]))\n",
    "    total_words_in_corpus+=total_words_in_class\n",
    "    print(f\"# of word-tokens in class {corpus.target_names[category_index]}: {total_words_in_class}\")                                                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae16657",
   "metadata": {},
   "source": [
    "7. h) the number of word-tokens in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "558d4ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in corpus: 672785\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total words in corpus: {total_words_in_corpus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb62881",
   "metadata": {},
   "source": [
    "7. i) the number and percentage of words with a frequency of zero in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c9895dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a frequency of zero in class business: 18693\n",
      "Percentage of words with a frequency of zero in class business: 14.265654214522838%\n",
      "\n",
      "Number of words with a frequency of zero in class entertainment: 18964\n",
      "Percentage of words with a frequency of zero in class entertainment: 18.511757757972727%\n",
      "\n",
      "Number of words with a frequency of zero in class politics: 19201\n",
      "Percentage of words with a frequency of zero in class politics: 13.150289017341041%\n",
      "\n",
      "Number of words with a frequency of zero in class sport: 19793\n",
      "Percentage of words with a frequency of zero in class sport: 14.80071786435355%\n",
      "\n",
      "Number of words with a frequency of zero in class tech: 18308\n",
      "Percentage of words with a frequency of zero in class tech: 11.473694105850281%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category_index in naive_bayes_classifier.classes_:\n",
    "    feature_count_only_zero = new_list = list(filter(lambda x: (x==0) , naive_bayes_classifier.feature_count_[category_index]))\n",
    "    number_of_zeroes_in_class=len(feature_count_only_zero)\n",
    "    total_words_in_class=int(sum(naive_bayes_classifier.feature_count_[category_index]))\n",
    "    print(f\"Number of words with a frequency of zero in class {corpus.target_names[category_index]}: {number_of_zeroes_in_class}\")\n",
    "    print(f\"Percentage of words with a frequency of zero in class {corpus.target_names[category_index]}: {(number_of_zeroes_in_class/total_words_in_class)*100}%\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8703050a",
   "metadata": {},
   "source": [
    "7. j) the number and percentage of words with a frequency of one in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2623d11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a frequency of one in corpus: 20954\n",
      "Percentage of words with a frequency of one in corpus: 3.1145165245955244%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_ones = 0\n",
    "for category_index in naive_bayes_classifier.classes_:\n",
    "    feature_count_only_ones = new_list = list(filter(lambda x: (x==1) , naive_bayes_classifier.feature_count_[category_index]))\n",
    "    number_of_ones_in_class=len(feature_count_only_ones)\n",
    "    sum_ones += number_of_ones_in_class\n",
    "\n",
    "print(f\"Number of words with a frequency of one in corpus: {sum_ones}\")\n",
    "print(f\"Percentage of words with a frequency of one in corpus: {(sum_ones/total_words_in_corpus)*100}%\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0635bb",
   "metadata": {},
   "source": [
    "7. k) your 2 favorite words (that are present in the vocabulary) and their log-prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cc95a1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.39551851053172\n",
      "-6.330864984866497\n"
     ]
    }
   ],
   "source": [
    "# Function only returns log of probability of a feature (f) given class (C) -- P(f|C)\n",
    "# To get probability of that feature  P(f)= Σ P(f)xP(f|C) \n",
    "fav1 = 'friday'\n",
    "fav2 = 'there'\n",
    "feat_names = vectorizer.get_feature_names()\n",
    "\n",
    "fav1_index = feat_names.index(fav1)\n",
    "fav2_index = feat_names.index(fav2)\n",
    "prob1 = 0\n",
    "prob2 = 0\n",
    "\n",
    "for i in range(5):\n",
    "    prob1 += (np.exp(naive_bayes_classifier.feature_log_prob_[i][fav1_index]))*(np.exp(naive_bayes_classifier.class_log_prior_[i]))\n",
    "    prob2 += (np.exp(naive_bayes_classifier.feature_log_prob_[i][fav2_index]))*(np.exp(naive_bayes_classifier.class_log_prior_[i]))  \n",
    "\n",
    "#convert probabilities to log-prob\n",
    "print(np.log(prob1)) \n",
    "print(np.log(prob2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63397a5",
   "metadata": {},
   "source": [
    "8. Redo steps 6 and 7 without changing anything (do not redo step 5, the dataset split). Change the model name to something like “MultinomialNB default values, try 2” and append the results to the file bbc-performance.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad38535",
   "metadata": {},
   "source": [
    "# Step 8: Redo steps 6 and 7 without changing anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3b1a0b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo of step 6 with default values\n",
    "naive_bayes_classifier_2 = MultinomialNB()\n",
    "naive_bayes_classifier_2.fit(X_train, y_train)\n",
    "\n",
    "y_naive_bayes_predict_2 = naive_bayes_classifier_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7a9cf01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[99  0  3  0  2]\n",
      " [ 0 70  1  0  2]\n",
      " [ 1  0 95  0  0]\n",
      " [ 0  0  1 87  0]\n",
      " [ 0  0  0  0 84]]\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7b\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_naive_bayes_predict_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ef5bb958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score:\n",
      "[0.99       1.         0.95       1.         0.95454545]\n",
      "\n",
      "recall score:\n",
      "[0.95192308 0.95890411 0.98958333 0.98863636 1.        ]\n",
      "\n",
      "f1-measure:\n",
      "[0.97058824 0.97902098 0.96938776 0.99428571 0.97674419]\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7c\n",
    "\n",
    "# precision score\n",
    "print(\"precision score:\")\n",
    "print(precision_score(y_test, y_naive_bayes_predict_2, average=None))\n",
    "\n",
    "# recall score\n",
    "print(\"\\nrecall score:\")\n",
    "print(recall_score(y_test, y_naive_bayes_predict_2, average=None))\n",
    "\n",
    "# f1-measure\n",
    "print(\"\\nf1-measure:\")\n",
    "print(f1_score(y_test, y_naive_bayes_predict_2, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "26f17706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:\n",
      "0.9775280898876404\n",
      "\n",
      "macro-average f1 score:\n",
      "0.9780053739498727\n",
      "\n",
      "weighted-average f1 score:\n",
      "0.9775608694695853\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7d\n",
    "\n",
    "# accuracy\n",
    "print(\"accuracy score:\")\n",
    "print(accuracy_score(y_test, y_naive_bayes_predict_2))\n",
    "\n",
    "# macro-average f1\n",
    "print(\"\\nmacro-average f1 score:\")\n",
    "print(f1_score(y_test, y_naive_bayes_predict_2, average='macro'))\n",
    "\n",
    "# weighted-average f1\n",
    "print(\"\\nweighted-average f1 score:\")\n",
    "print(f1_score(y_test, y_naive_bayes_predict_2, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "daa09ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(category) = # of documents in category / total # of documents\n",
      "\n",
      "P(business) = 0.22808988764044943\n",
      "P(entertainment) = 0.17584269662921348\n",
      "P(politics) = 0.18033707865168538\n",
      "P(sport) = 0.23764044943820226\n",
      "P(tech) = 0.17808988764044945\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7e\n",
    "\n",
    "# Calculate total # of documents\n",
    "total_num_of_documents_2 = sum(naive_bayes_classifier_2.class_count_);\n",
    "print(\"P(category) = # of documents in category / total # of documents\\n\")\n",
    "    \n",
    "for category_index in naive_bayes_classifier_2.classes_:\n",
    "    print(f\"P({corpus.target_names[category_index]}) = {naive_bayes_classifier_2.class_count_[category_index]/total_num_of_documents_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6e2454bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 29421\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7f\n",
    "\n",
    "#Vocabulary\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "359ece65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of word-tokens in class business: 131035\n",
      "# of word-tokens in class entertainment: 102443\n",
      "# of word-tokens in class politics: 146012\n",
      "# of word-tokens in class sport: 133730\n",
      "# of word-tokens in class tech: 159565\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7g\n",
    "\n",
    "total_words_in_corpus_2=0\n",
    "for category_index in naive_bayes_classifier_2.classes_:\n",
    "    total_words_in_class_2=int(sum(naive_bayes_classifier_2.feature_count_[category_index]))\n",
    "    total_words_in_corpus_2+=total_words_in_class_2\n",
    "    print(f\"# of word-tokens in class {corpus.target_names[category_index]}: {total_words_in_class_2}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ac1a938a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in corpus: 672785\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7h\n",
    "print(f\"Total words in corpus: {total_words_in_corpus_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5209e52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a frequency of zero in class business: 18693\n",
      "Percentage of words with a frequency of zero in class business: 14.265654214522838%\n",
      "\n",
      "Number of words with a frequency of zero in class entertainment: 18964\n",
      "Percentage of words with a frequency of zero in class entertainment: 18.511757757972727%\n",
      "\n",
      "Number of words with a frequency of zero in class politics: 19201\n",
      "Percentage of words with a frequency of zero in class politics: 13.150289017341041%\n",
      "\n",
      "Number of words with a frequency of zero in class sport: 19793\n",
      "Percentage of words with a frequency of zero in class sport: 14.80071786435355%\n",
      "\n",
      "Number of words with a frequency of zero in class tech: 18308\n",
      "Percentage of words with a frequency of zero in class tech: 11.473694105850281%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7i\n",
    "for category_index in naive_bayes_classifier_2.classes_:\n",
    "    feature_count_only_zero_2 = new_list_2 = list(filter(lambda x: (x==0) , naive_bayes_classifier_2.feature_count_[category_index]))\n",
    "    number_of_zeroes_in_class_2=len(feature_count_only_zero_2)\n",
    "    total_words_in_class_2=int(sum(naive_bayes_classifier_2.feature_count_[category_index]))\n",
    "    print(f\"Number of words with a frequency of zero in class {corpus.target_names[category_index]}: {number_of_zeroes_in_class_2}\")\n",
    "    print(f\"Percentage of words with a frequency of zero in class {corpus.target_names[category_index]}: {(number_of_zeroes_in_class_2/total_words_in_class_2)*100}%\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "822a1de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a frequency of one in corpus: 20954\n",
      "Percentage of words with a frequency of one in corpus: 3.1145165245955244%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7j\n",
    "\n",
    "sum_ones_2 = 0\n",
    "for category_index in naive_bayes_classifier_2.classes_:\n",
    "    feature_count_only_ones_2 = new_list_2 = list(filter(lambda x: (x==1) , naive_bayes_classifier_2.feature_count_[category_index]))\n",
    "    number_of_ones_in_class_2=len(feature_count_only_ones_2)\n",
    "    sum_ones_2 += number_of_ones_in_class_2\n",
    "\n",
    "print(f\"Number of words with a frequency of one in corpus: {sum_ones_2}\")\n",
    "print(f\"Percentage of words with a frequency of one in corpus: {(sum_ones_2/total_words_in_corpus_2)*100}%\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6319ac48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.39551851053172\n",
      "-6.330864984866497\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7k\n",
    "\n",
    "# Function only returns log of probability of a feature (f) given class (C) -- P(f|C)\n",
    "# To get probability of that feature  P(f)= Σ P(f)xP(f|C) \n",
    "fav1 = 'friday'\n",
    "fav2 = 'there'\n",
    "feat_names = vectorizer.get_feature_names()\n",
    "\n",
    "fav1_index = feat_names.index(fav1)\n",
    "fav2_index = feat_names.index(fav2)\n",
    "prob1_2 = 0\n",
    "prob2_2 = 0\n",
    "\n",
    "for i in range(5):\n",
    "    prob1_2 += (np.exp(naive_bayes_classifier_2.feature_log_prob_[i][fav1_index]))*(np.exp(naive_bayes_classifier_2.class_log_prior_[i]))\n",
    "    prob2_2 += (np.exp(naive_bayes_classifier_2.feature_log_prob_[i][fav2_index]))*(np.exp(naive_bayes_classifier_2.class_log_prior_[i]))  \n",
    "\n",
    "#convert probabilities to log-prob\n",
    "print(np.log(prob1_2)) \n",
    "print(np.log(prob2_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cc0187",
   "metadata": {},
   "source": [
    "9. Redo steps 6 and 7 again, but this time, change the smoothing value to 0.0001. Append the results at the end of bbc-performance.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eacac4",
   "metadata": {},
   "source": [
    "# Step 9: Redo steps 6 and 7 again, but this time, change the smoothing value to 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4369dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo of step 6 with smoothing value of 0.0001\n",
    "naive_bayes_classifier_3 = MultinomialNB(alpha=0.0001)\n",
    "naive_bayes_classifier_3.fit(X_train, y_train)\n",
    "\n",
    "y_naive_bayes_predict_3 = naive_bayes_classifier_3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "080556d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[100   0   2   0   2]\n",
      " [  0  71   0   0   2]\n",
      " [  2   0  94   0   0]\n",
      " [  0   0   1  87   0]\n",
      " [  1   0   0   0  83]]\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7b\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_naive_bayes_predict_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "54967a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score:\n",
      "[0.97087379 1.         0.96907216 1.         0.95402299]\n",
      "\n",
      "recall score:\n",
      "[0.96153846 0.97260274 0.97916667 0.98863636 0.98809524]\n",
      "\n",
      "f1-measure:\n",
      "[0.96618357 0.98611111 0.97409326 0.99428571 0.97076023]\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7c\n",
    "\n",
    "# precision score\n",
    "print(\"precision score:\")\n",
    "print(precision_score(y_test, y_naive_bayes_predict_3, average=None))\n",
    "\n",
    "# recall score\n",
    "print(\"\\nrecall score:\")\n",
    "print(recall_score(y_test, y_naive_bayes_predict_3, average=None))\n",
    "\n",
    "# f1-measure\n",
    "print(\"\\nf1-measure:\")\n",
    "print(f1_score(y_test, y_naive_bayes_predict_3, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "aa0f6a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:\n",
      "0.9775280898876404\n",
      "\n",
      "macro-average f1 score:\n",
      "0.9782867796885771\n",
      "\n",
      "weighted-average f1 score:\n",
      "0.9775801320734653\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7d\n",
    "\n",
    "# accuracy\n",
    "print(\"accuracy score:\")\n",
    "print(accuracy_score(y_test, y_naive_bayes_predict_3))\n",
    "\n",
    "# macro-average f1\n",
    "print(\"\\nmacro-average f1 score:\")\n",
    "print(f1_score(y_test, y_naive_bayes_predict_3, average='macro'))\n",
    "\n",
    "# weighted-average f1\n",
    "print(\"\\nweighted-average f1 score:\")\n",
    "print(f1_score(y_test, y_naive_bayes_predict_3, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1aea1820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(category) = # of documents in category / total # of documents\n",
      "\n",
      "P(business) = 0.22808988764044943\n",
      "P(entertainment) = 0.17584269662921348\n",
      "P(politics) = 0.18033707865168538\n",
      "P(sport) = 0.23764044943820226\n",
      "P(tech) = 0.17808988764044945\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7e\n",
    "\n",
    "# Calculate total # of documents\n",
    "total_num_of_documents_3 = sum(naive_bayes_classifier_3.class_count_);\n",
    "print(\"P(category) = # of documents in category / total # of documents\\n\")\n",
    "    \n",
    "for category_index in naive_bayes_classifier_3.classes_:\n",
    "    print(f\"P({corpus.target_names[category_index]}) = {naive_bayes_classifier_3.class_count_[category_index]/total_num_of_documents_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c1193700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 29421\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7f\n",
    "\n",
    "#Vocabulary\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ae72974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of word-tokens in class business: 131035\n",
      "# of word-tokens in class entertainment: 102443\n",
      "# of word-tokens in class politics: 146012\n",
      "# of word-tokens in class sport: 133730\n",
      "# of word-tokens in class tech: 159565\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7g\n",
    "\n",
    "total_words_in_corpus_3=0\n",
    "for category_index in naive_bayes_classifier_3.classes_:\n",
    "    total_words_in_class_3=int(sum(naive_bayes_classifier_3.feature_count_[category_index]))\n",
    "    total_words_in_corpus_3+=total_words_in_class_3\n",
    "    print(f\"# of word-tokens in class {corpus.target_names[category_index]}: {total_words_in_class_3}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ec88ccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in corpus: 672785\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7h\n",
    "print(f\"Total words in corpus: {total_words_in_corpus_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e109e55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a frequency of zero in class business: 18693\n",
      "Percentage of words with a frequency of zero in class business: 14.265654214522838%\n",
      "\n",
      "Number of words with a frequency of zero in class entertainment: 18964\n",
      "Percentage of words with a frequency of zero in class entertainment: 18.511757757972727%\n",
      "\n",
      "Number of words with a frequency of zero in class politics: 19201\n",
      "Percentage of words with a frequency of zero in class politics: 13.150289017341041%\n",
      "\n",
      "Number of words with a frequency of zero in class sport: 19793\n",
      "Percentage of words with a frequency of zero in class sport: 14.80071786435355%\n",
      "\n",
      "Number of words with a frequency of zero in class tech: 18308\n",
      "Percentage of words with a frequency of zero in class tech: 11.473694105850281%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7i\n",
    "for category_index in naive_bayes_classifier_3.classes_:\n",
    "    feature_count_only_zero_3 = new_list_3 = list(filter(lambda x: (x==0) , naive_bayes_classifier_3.feature_count_[category_index]))\n",
    "    number_of_zeroes_in_class_3=len(feature_count_only_zero_3)\n",
    "    total_words_in_class_3=int(sum(naive_bayes_classifier_3.feature_count_[category_index]))\n",
    "    print(f\"Number of words with a frequency of zero in class {corpus.target_names[category_index]}: {number_of_zeroes_in_class_3}\")\n",
    "    print(f\"Percentage of words with a frequency of zero in class {corpus.target_names[category_index]}: {(number_of_zeroes_in_class_3/total_words_in_class_3)*100}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d314f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a frequency of one in corpus: 20954\n",
      "Percentage of words with a frequency of one in corpus: 3.1145165245955244%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7j\n",
    "\n",
    "sum_ones_3 = 0\n",
    "for category_index in naive_bayes_classifier_3.classes_:\n",
    "    feature_count_only_ones_3 = new_list_3 = list(filter(lambda x: (x==1) , naive_bayes_classifier_3.feature_count_[category_index]))\n",
    "    number_of_ones_in_class_3=len(feature_count_only_ones_3)\n",
    "    sum_ones_3 += number_of_ones_in_class_3\n",
    "\n",
    "print(f\"Number of words with a frequency of one in corpus: {sum_ones_3}\")\n",
    "print(f\"Percentage of words with a frequency of one in corpus: {(sum_ones_3/total_words_in_corpus_3)*100}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d3142344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.214434302022129\n",
      "-6.136728869371583\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7k\n",
    "\n",
    "# Function only returns log of probability of a feature (f) given class (C) -- P(f|C)\n",
    "# To get probability of that feature  P(f)= Σ P(f)xP(f|C) \n",
    "fav1 = 'friday'\n",
    "fav2 = 'there'\n",
    "feat_names = vectorizer.get_feature_names()\n",
    "\n",
    "fav1_index = feat_names.index(fav1)\n",
    "fav2_index = feat_names.index(fav2)\n",
    "prob1_3 = 0\n",
    "prob2_3 = 0\n",
    "\n",
    "for i in range(5):\n",
    "    prob1_3 += (np.exp(naive_bayes_classifier_3.feature_log_prob_[i][fav1_index]))*(np.exp(naive_bayes_classifier_3.class_log_prior_[i]))\n",
    "    prob2_3 += (np.exp(naive_bayes_classifier_3.feature_log_prob_[i][fav2_index]))*(np.exp(naive_bayes_classifier_3.class_log_prior_[i]))  \n",
    "\n",
    "#convert probabilities to log-prob\n",
    "print(np.log(prob1_3)) \n",
    "print(np.log(prob2_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f560a7",
   "metadata": {},
   "source": [
    "10. Redo steps 6 and 7, but this time, change the smoothing value to 0.9. Append the results at the end of bbc-performance.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407d5aab",
   "metadata": {},
   "source": [
    "# Step 10: Redo steps 6 and 7, but this time, change the smoothing value to 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8ff38e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo of step 6 with smoothing value of 0.9\n",
    "naive_bayes_classifier_4 = MultinomialNB(alpha=0.9)\n",
    "naive_bayes_classifier_4.fit(X_train, y_train)\n",
    "\n",
    "y_naive_bayes_predict_4 = naive_bayes_classifier_4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a05aaa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[99  0  3  0  2]\n",
      " [ 0 70  1  0  2]\n",
      " [ 1  0 95  0  0]\n",
      " [ 0  0  1 87  0]\n",
      " [ 0  0  0  0 84]]\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7b\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_naive_bayes_predict_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1b6b5f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score:\n",
      "[0.99       1.         0.95       1.         0.95454545]\n",
      "\n",
      "recall score:\n",
      "[0.95192308 0.95890411 0.98958333 0.98863636 1.        ]\n",
      "\n",
      "f1-measure:\n",
      "[0.97058824 0.97902098 0.96938776 0.99428571 0.97674419]\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7c\n",
    "\n",
    "# precision score\n",
    "print(\"precision score:\")\n",
    "print(precision_score(y_test, y_naive_bayes_predict_4, average=None))\n",
    "\n",
    "# recall score\n",
    "print(\"\\nrecall score:\")\n",
    "print(recall_score(y_test, y_naive_bayes_predict_4, average=None))\n",
    "\n",
    "# f1-measure\n",
    "print(\"\\nf1-measure:\")\n",
    "print(f1_score(y_test, y_naive_bayes_predict_4, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c275784e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:\n",
      "0.9775280898876404\n",
      "\n",
      "macro-average f1 score:\n",
      "0.9780053739498727\n",
      "\n",
      "weighted-average f1 score:\n",
      "0.9775608694695853\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7d\n",
    "\n",
    "# accuracy\n",
    "print(\"accuracy score:\")\n",
    "print(accuracy_score(y_test, y_naive_bayes_predict_4))\n",
    "\n",
    "# macro-average f1\n",
    "print(\"\\nmacro-average f1 score:\")\n",
    "print(f1_score(y_test, y_naive_bayes_predict_4, average='macro'))\n",
    "\n",
    "# weighted-average f1\n",
    "print(\"\\nweighted-average f1 score:\")\n",
    "print(f1_score(y_test, y_naive_bayes_predict_4, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f24bd061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(category) = # of documents in category / total # of documents\n",
      "\n",
      "P(business) = 0.22808988764044943\n",
      "P(entertainment) = 0.17584269662921348\n",
      "P(politics) = 0.18033707865168538\n",
      "P(sport) = 0.23764044943820226\n",
      "P(tech) = 0.17808988764044945\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7e\n",
    "\n",
    "# Calculate total # of documents\n",
    "total_num_of_documents_4 = sum(naive_bayes_classifier_4.class_count_);\n",
    "print(\"P(category) = # of documents in category / total # of documents\\n\")\n",
    "    \n",
    "for category_index in naive_bayes_classifier_4.classes_:\n",
    "    print(f\"P({corpus.target_names[category_index]}) = {naive_bayes_classifier_4.class_count_[category_index]/total_num_of_documents_4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7a586147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 29421\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7f\n",
    "\n",
    "#Vocabulary\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "81fd1a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of word-tokens in class business: 131035\n",
      "# of word-tokens in class entertainment: 102443\n",
      "# of word-tokens in class politics: 146012\n",
      "# of word-tokens in class sport: 133730\n",
      "# of word-tokens in class tech: 159565\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7g\n",
    "\n",
    "total_words_in_corpus_4=0\n",
    "for category_index in naive_bayes_classifier_4.classes_:\n",
    "    total_words_in_class_4=int(sum(naive_bayes_classifier_4.feature_count_[category_index]))\n",
    "    total_words_in_corpus_4+=total_words_in_class_4\n",
    "    print(f\"# of word-tokens in class {corpus.target_names[category_index]}: {total_words_in_class_4}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4ae1ab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in corpus: 672785\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7h\n",
    "print(f\"Total words in corpus: {total_words_in_corpus_4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2e7f3279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a frequency of zero in class business: 18693\n",
      "Percentage of words with a frequency of zero in class business: 14.265654214522838%\n",
      "\n",
      "Number of words with a frequency of zero in class entertainment: 18964\n",
      "Percentage of words with a frequency of zero in class entertainment: 18.511757757972727%\n",
      "\n",
      "Number of words with a frequency of zero in class politics: 19201\n",
      "Percentage of words with a frequency of zero in class politics: 13.150289017341041%\n",
      "\n",
      "Number of words with a frequency of zero in class sport: 19793\n",
      "Percentage of words with a frequency of zero in class sport: 14.80071786435355%\n",
      "\n",
      "Number of words with a frequency of zero in class tech: 18308\n",
      "Percentage of words with a frequency of zero in class tech: 11.473694105850281%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7i\n",
    "for category_index in naive_bayes_classifier_4.classes_:\n",
    "    feature_count_only_zero_4 = new_list_4 = list(filter(lambda x: (x==0) , naive_bayes_classifier_4.feature_count_[category_index]))\n",
    "    number_of_zeroes_in_class_4=len(feature_count_only_zero_4)\n",
    "    total_words_in_class_4=int(sum(naive_bayes_classifier_4.feature_count_[category_index]))\n",
    "    print(f\"Number of words with a frequency of zero in class {corpus.target_names[category_index]}: {number_of_zeroes_in_class_4}\")\n",
    "    print(f\"Percentage of words with a frequency of zero in class {corpus.target_names[category_index]}: {(number_of_zeroes_in_class_4/total_words_in_class_4)*100}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b41ad8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a frequency of one in corpus: 20954\n",
      "Percentage of words with a frequency of one in corpus: 3.1145165245955244%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7j\n",
    "\n",
    "sum_ones_4 = 0\n",
    "for category_index in naive_bayes_classifier_4.classes_:\n",
    "    feature_count_only_ones_4 = new_list_4 = list(filter(lambda x: (x==1) , naive_bayes_classifier_4.feature_count_[category_index]))\n",
    "    number_of_ones_in_class_4=len(feature_count_only_ones_4)\n",
    "    sum_ones_4 += number_of_ones_in_class_4\n",
    "\n",
    "print(f\"Number of words with a frequency of one in corpus: {sum_ones_4}\")\n",
    "print(f\"Percentage of words with a frequency of one in corpus: {(sum_ones_4/total_words_in_corpus_4)*100}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9c3f8ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.37928708961673\n",
      "-6.313161738180073\n"
     ]
    }
   ],
   "source": [
    "# Redo of 7k\n",
    "\n",
    "# Function only returns log of probability of a feature (f) given class (C) -- P(f|C)\n",
    "# To get probability of that feature  P(f)= Σ P(f)xP(f|C) \n",
    "fav1 = 'friday'\n",
    "fav2 = 'there'\n",
    "feat_names = vectorizer.get_feature_names()\n",
    "\n",
    "fav1_index = feat_names.index(fav1)\n",
    "fav2_index = feat_names.index(fav2)\n",
    "prob1_4 = 0\n",
    "prob2_4 = 0\n",
    "\n",
    "for i in range(5):\n",
    "    prob1_4 += (np.exp(naive_bayes_classifier_4.feature_log_prob_[i][fav1_index]))*(np.exp(naive_bayes_classifier_4.class_log_prior_[i]))\n",
    "    prob2_4 += (np.exp(naive_bayes_classifier_4.feature_log_prob_[i][fav2_index]))*(np.exp(naive_bayes_classifier_4.class_log_prior_[i]))  \n",
    "\n",
    "#convert probabilities to log-prob\n",
    "print(np.log(prob1_4)) \n",
    "print(np.log(prob2_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646bf4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
