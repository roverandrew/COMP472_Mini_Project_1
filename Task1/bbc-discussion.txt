11.
a) Since we care about the proportion of correct classifications over the predicted classifications, precision would be a good measure for this dataset. But since the number of documents in each class differs, we need to take the weighted precision to account for this. Weighted precision would be the best metric for this data. (If we are talking about only the metrics we calculated in this assignment, the best metric would actually be weighted F1-measure, since we never calculate the weighted precision.)

b) The performance of step 8 is the same as step 7 because it is running the Multinomial NB on the same training set and testing on the same test set as step 7, all while using the same default parameters in the MulitnomialNB(). 
The performance of step 9 was improved since we lowered the smoothing value for the Multinomial NB down from 1 to 0.0001. We can see the performance was improved by observing increased values for all the metrics; confusion matrix, accuracy, precision, recall, f1-measure. Lowering the smoothing value improved performance because this type of data is so noisy. There are many words that appear across multiple classes, thus clouding the data already. There are also certain words that only appear in certain classes, which is why adding a smoothing value would slightly decrease performance since those words would be a good indicator to predict the right class.
The performance of step 10 was the same as for step 7 since the smoothing value was 0.9, only a slight decrease from 1.0.