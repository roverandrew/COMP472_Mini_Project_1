**********************************************************************************************************************************

PART 7

***************************************************************

Gaussian Naive Bayes Classifier 

a) Uses default parameters

b) 
[[ 4  0  0  0  0]
 [ 0  3  0  0  0]
 [ 0  0  3  0  0]
 [ 0  0  0 11  0]
 [ 6  2  5  6 10]]
 
c) 
Precision: [0.4        0.6        0.375      0.64705882 1.        ]
Recall: [1.         1.         1.         1.         0.34482759]
F1-Measure: [0.57142857 0.75       0.54545455 0.78571429 0.51282051]

d)
Accuracy: 0.62
Macro-Average F1: 0.633083583083583
Weighted-Average F1: 0.5937345987345988


***************************************************************

Base Decision Tree
a) Uses default parameters

b)
[[ 4  0  0  0  0]
 [ 0  3  0  0  0]
 [ 0  0  3  0  0]
 [ 0  0  0 11  0]
 [ 0  0  0  0 29]]
 
c) 
Precision: [1. 1. 1. 1. 1.]
Recall: [1. 1. 1. 1. 1.]
F1-Measure: [1. 1. 1. 1. 1.]

d)
Accuracy: 1.0
Macro-Average F1: 1.0
Weighted-Average F1: 1.0


***************************************************************

Top Decision Tree
a) 
Hyperparameters changed:
params = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 17],
    'min_samples_split': [2, 8, 18]
}
Best hyperparameters: DecisionTreeClassifier(max_depth=5)

b) 
[[ 4  0  0  0  0]
 [ 0  3  0  0  0]
 [ 0  0  3  0  0]
 [ 0  0  0 11  0]
 [ 0  0  0  0 29]]
 
c) 
Precision: [1. 1. 1. 1. 1.]
Recall: [1. 1. 1. 1. 1.]
F1-Measure: [1. 1. 1. 1. 1.]

d)
Accuracy: 1.0
Macro-Average F1: 1.0
Weighted-Average F1: 1.0


***************************************************************

Perceptron
a) Uses default parameters

b) 
[[ 0  0  0  4  0]
 [ 0  0  0  3  0]
 [ 0  0  0  2  1]
 [ 0  0  0 10  1]
 [ 0  0  0 13 16]]
 
c) 
Precision: [0.         0.         0.         0.3125     0.88888889]
Recall: [0.         0.         0.         0.90909091 0.55172414]
F1-Measure: [0.         0.         0.         0.46511628 0.68085106]

d)
Accuracy: 0.52
Macro-Average F1: 0.22919346857991094
Weighted-Average F1: 0.49721919841662543


***************************************************************

Base Multi-Layered Perceptron Decision Tree
a) 
Parameters: hidden_layer_sizes=(1,100), activation='logistic', solver='sgd'

b) 
[[ 0  0  0  0  4]
 [ 0  0  0  0  3]
 [ 0  0  0  0  3]
 [ 0  0  0  0 11]
 [ 0  0  0  0 29]]
 
c) 
Precision: [0.   0.   0.   0.   0.58]
Recall: [0. 0. 0. 0. 1.]
F1-Measure: [0.         0.         0.         0.         0.73417722]

d)
Accuracy: 0.58
Macro-Average F1: 0.14683544303797466
Weighted-Average F1: 0.42582278481012653


***************************************************************

Top Multi-Layered Perceptron
a) 
Hyperparameters changed:
params = {
    'activation': ['logistic', 'tanh', 'relu', 'identity'],
    'hidden_layer_sizes': [(2, 35), (3, 10)],
    'solver': ['adam', 'sgd']
}
Best hyperparameters: MLPClassifier(activation='identity', hidden_layer_sizes=(2, 35))


b) 
[[ 0  0  0  2  2]
 [ 0  0  0  3  0]
 [ 0  1  0  0  2]
 [ 0  0  0  6  5]
 [ 0  0  0  7 22]]

c) 
Precision: [0.         0.         0.         0.33333333 0.70967742]
Recall: [0.         0.         0.         0.54545455 0.75862069]
F1-Measure: [0.         0.         0.         0.4137931  0.73333333]

d)
Accuracy: 0.56
Macro-Average F1: 0.22942528735632184
Weighted-Average F1: 0.516367816091954


**********************************************************************************************************************************

PART 8

***************************************************************
Gaussian Naive Bayes Classifier 

Accuracy: 0.62, 0.72, 0.74, 0.72, 0.74, 0.72, 0.68, 0.72, 0.78, 0.68, 
Macro-Average F1: 0.633083583083583, 0.6817063492063492, 0.7601251646903822, 0.7276470588235293, 6792673992673992, 0.736541889483066, 0.6411111111111111, 0.6606015037593986, 0.7437837837837837, 0.6989107666527021, 
Weighted-Average F1: 0.5937345987345988, 0.7006230158730159, 0.7182009222661396, 0.7080053475935828, 6934212454212455, 0.7064527629233511, 0.6571111111111111, 0.725468671679198, 0.7684324324324323, 0.6268177628822791, 

Averages
Accuracy: 0.712
Macro-Average F1: 0.6981679123
Weighted-Average F1: 0.6894274028

Standard Deviations
Accuracy: 0.04341018826
Macro-Average F1: 0.04668419095
Weighted-Average F1: 0.05391016872


***************************************************************

Base Decision Tree

Accuracy: 1.0, 0.98, 1.0, 0.98, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 
Macro-Average F1: 1.0, 0.9892930679478381, 1.0, 0.9595959595959596, 0.9595959595959596, 1.0, 0.9890218156228009, 1.0, 1.0, 1.0, 1.0, 
Weighted-Average F1: 1.0, 0.9798901853122856, 1.0, 0.9797979797979798, 0.9797979797979798, 1.0, 0.9798592540464461, 1.0, 1.0, 1.0, 1.0,

Averages
Accuracy: 0.994
Macro-Average F1: 0.9897506803
Weighted-Average F1: 0.9919345399

Standard Deviations
Accuracy: 0.009660917831
Macro-Average F1: 0.01649796735
Weighted-Average F1: 0.01041249819


***************************************************************

Top Decision Tree

Accuracy: 1.0, 0.98, 1.0, 0.98, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 
Macro-Average F1: 1.0, 0.9892930679478381, 1.0, 0.9595959595959596, 1.0, 0.9890218156228009, 1.0, 1.0, 1.0, 1.0, 
Weighted-Average F1: 1.0, 0.9798901853122856, 1.0, 0.9797979797979798, 1.0, 0.9798592540464461, 1.0, 1.0, 1.0, 1.0, 

Averages
Accuracy: 0.994
Macro-Average F1: 0.9937910843
Weighted-Average F1: 0.9939547419

Standard Deviations
Accuracy: 0.009660917831
Macro-Average F1: 0.0128328531
Weighted-Average F1: 0.009733815408


***************************************************************

Perceptron

Accuracy: 0.52, 0.5, 0.5, 0.48, 0.34, 0.52, 0.32, 0.52, 0.46, 0.5, 
Macro-Average F1: 0.22919346857991094, 0.17402190923317684, 0.1873134328358209, 0.21944444444444447, 0.10149253731343284, 0.23733333333333334, 0.1982683982683983, 0.22800000000000004, 0.17839080459770115, 0.2242622950819672, 
Weighted-Average F1: 0.49721919841662543, 0.36913928012519565, 0.36582089552238806, 0.4113888888888889, 0.17253731343283582, 0.47600000000000003, 0.33298701298701294, 0.4728, 0.33645977011494255, 0.34937704918032786, 

Averages
Accuracy: 0.466
Macro-Average F1: 0.1977720624
Weighted-Average F1: 0.3783729409

Standard Deviations
Accuracy: 0.07426678636
Macro-Average F1: 0.04076576841
Weighted-Average F1: 0.09482760724


***************************************************************

Base Multi-Layered Perceptron Decision Tree

Accuracy: 0.58, 0.46, 0.46, 0.46, 0.34, 0.48, 0.48, 0.52, 0.44, 0.44, 
Macro-Average F1: 0.14683544303797466, 0.12602739726027398, 0.12602739726027398, , 0.12602739726027398, 0.10149253731343284, 0.12972972972972974, 0.12972972972972974, 0.1368421052631579, 0.12222222222222223, 0.12222222222222223, 
Weighted-Average F1: 0.42582278481012653, 0.28986301369863016, 0.28986301369863016, 0.28986301369863016, 0.17253731343283582, 0.3113513513513514, 0.3113513513513514, 0.35578947368421054, 0.26888888888888896, 0.26888888888888896, 

Averages
Accuracy: 0.466
Macro-Average F1: 0.1272148843
Weighted-Average F1: 0.2984219094

Standard Deviations
Accuracy: 0.0611373681
Macro-Average F1: 0.01215059047
Weighted-Average F1: 0.06469368475


***************************************************************

Top Multi-Layered Perceptron

Accuracy: 0.56, 0.74, 0.48, 0.58, 0.46, 0.64, 0.6, 0.58, 0.76, 0.44, 
Macro-Average F1: 0.22942528735632184, 0.3282744282744282, 0.20211640211640214, 0.2702127659574468, 0.2517748917748918, 0.28213903743315505, 0.3652291105121294, 0.2111111111111111, 0.5266106442577031, 0.12941176470588237, 
Weighted-Average F1: 0.516367816091954, 0.6490852390852391, 0.38783068783068786, 0.5014893617021277, 0.41710822510822515, 0.560663101604278, 0.5451212938005391, 0.46888888888888886, 0.7014005602240897, 0.2847058823529412

Averages
Accuracy: 0.584
Macro-Average F1: 0.2796305443
Weighted-Average F1: 0.5032661057

Standard Deviations
Accuracy: 0.1086482602
Macro-Average F1: 0.1092159129
Weighted-Average F1: 0.1226741146

